[{"id":0,"href":"/components/argo/","title":"Argo","section":"Components","content":"Argo Workflows Component #  Description #  Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD.\n Define workflows where each step in the workflow is a container. Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a graph (DAG). Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo Workflows on Kubernetes. Run CI/CD pipelines natively on Kubernetes without configuring complex software development products.  Implementation Details \u0026amp; Parameters #  The Argo Workflows Component has the following directory structure:\n./ ├── hub-component.yaml # configuration and parameters file of Hub component ├── values.yaml.template # template of Helm\u0026#39;s values.yaml file ├── post-deploy # script that is executed after deploy of the current component ├── post-undeploy # script that is executed after undeploy of the current component ├── pre-deploy # script that is executed before deploy of the current component ├── charts/ # directory for Helm Chart sources ├── resources/ # component custom resource descriptors └── backup # shell script that contains backup routines The component uses an offical Helm Chart to provision Argo Forkflows.\nTo make configuration more flexible, ingress object for Argo workflow dashboard and IAM is configured outside the Helm in post-deploy script.\nThe following component level parameters can be set in hub-component.yaml:\n   Name Description Default Value     component.argo.namespace Kubernetes namespace where Argo is provisioned kubeflow   component.argo.workflowNamespace Kubernetes namespace where workflow workloads are created kubeflow   component.argo.workflowRBAC Flag that enables k8s Role and RoleBinding creation in the workflowNamespace with required permissions to run workflow workloads true   component.argo.version Argo version v2.12.3   component.argo.containerRuntimeExecutor Container runtime, read more here k8sapi   component.argo.helm.chart Helm Chart version argo-workflows-0.9.4.tgz    Dependencies #  Argo Component does not depend on any other component in the stack\nReferences #   Argo Workflows Helm  "},{"id":1,"href":"/components/pipelines/","title":"Pipelines","section":"Components","content":"Kubeflow pipelines #  Overview of the Kubeflow pipelines service #  Kubeflow is a machine learning (ML) toolkit that is dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable.\nKubeflow pipelines are reusable end-to-end ML workflows built using the Kubeflow Pipelines SDK.\nThe Kubeflow pipelines service has the following goals:\n End to end orchestration: enabling and simplifying the orchestration of end to end machine learning pipelines Easy experimentation: making it easy for you to try numerous ideas and techniques, and manage your various trials/experiments. Easy re-use: enabling you to re-use components and pipelines to quickly cobble together end to end solutions, without having to re-build each time.  "},{"id":2,"href":"/stacks/kubeflow-gcp/","title":"Kubeflow Gcp","section":"Kubeflow","content":"Kubeflow Stack on GCP #  Here we explain how to deploy a Kubeflow into your Google Cloud Platform environment\nDeployment Prerequisites #   You should be signed in and have an active project in GCP: https://console.cloud.google.com You should have already deployed a GKE cluster.   Don\u0026rsquo;t have a cluster? Not a problem. We have a GKE stack for you, follow this link\n Deployment #  Deployment via Cloud Shell #  The easiest way how to get started is via Cloud Shell Edititor. Choose from the list of available Kubeflow stacks\n   Kubeflow Stack Description Link     Kubeflow cluster v1.2 This is our latest Kubeflow stack available     Start a new cloud shell session #  What will happen when you click to the button:\n You will start a new Cloud Shell editor session (best works with the Chrome Browser) Cloud shell will use a Shellbox image with all tools needed to deploy a kubeflow. No additional configuration required, See shellbox on gcr Cloud shell will clone a git repo with the stack files: See github repo: kubeflow-stacks  Init stack #  First you need initialize the sandbox configuration. To do this please run the initialization command:\nhub stack init The command will:\n Download the components described in hub.yaml Configure this Cloud Shell session for your GCP. It will ask other GCP essentials. It will take Region and Location (zone) from GCP metadat aserver. You can change settings by modifying .env file  Configure and Deploy a stack #  Once you are done with the configuration, use the following command to deploy the sandbox:\nhub stack deploy Before the first deployment you will be prompted for few questions. Your settings will be captured in .env file.\n Name of GKE cluster in your region. You will see a list of already deployed GKE clusters in your region. For different region, please adjust variables in .env file and thern run hub stack deploy command again Name of the storage class (more info here) GKE provides several storage classes you will see them Other questions\u0026hellip;  Once all configuration settings completed it will start the deployment automatically\nReview configuration parameters before deployment #  Wait, not so fast! I want to review coniguration settings (and possibly adjust) before the deployment.\nIn this case, run the following command\nhub stack configure This command will only do a configuration step and save results in .env file. So you can review and adjust. Once you are happy then run\nhub stack deploy Clean Up #  To remove your Kubeflow deployment, please run the following command\nhub stack undeploy "},{"id":3,"href":"/components/","title":"Components","section":"","content":"List of components #   Pipelines  "},{"id":4,"href":"/stacks/","title":"Kubeflow","section":"","content":"Available Kubeflow Stacks #  TBD\n"}]