[{"id":0,"href":"/features/multi-tenancy/","title":"Multi Tenancy","section":"Features","content":"Multi-Tenancy #  Multi-user isolation and identity access management (IAM)\nComming soon #  "},{"id":1,"href":"/features/notebooks/","title":"Notebooks","section":"Features","content":"Kubeflow notebooks #  Kubeflow Notebooks provides a way to run web-based development environments inside your Kubernetes cluster by running them inside Pods.\nComming soon #  "},{"id":2,"href":"/features/pipelines/","title":"Pipelines","section":"Features","content":"Kubeflow Pipelines #  Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on Docker containers.\nComming soon #  "},{"id":3,"href":"/getting-started/deploy-kubeflow-on-cloud/","title":"Deploy Kubeflow on Cloud","section":"Getting Started","content":"Kubeflow #  Kubeflow has been available via stacks of components. At present there is only one stack (for GCP), yet more will come in the near future.\nDeployment Prerequisites #   You should be signed in and have an active project in GCP: https://console.cloud.google.com You should have already deployed a GKE cluster   Note: there is a separate stack for GKE. It can act as an example, please feel free to customize the stack and deploy it: link\n Quick Start #  Choose a kubeflow template and run a cloud shell session.\nhub stack init hub stack deploy Or the following, if you want to review settings before deployment.\nhub stack init hub stack configure hub stack deploy List of pre-built Kubeflow stacks deployable via GCP Cloud Shell.\n   Kubeflow Stack Description Link     Kubeflow cluster v1.2 This is our latest Kubeflow stack available     Under the Hood #  Cloud Shell Editor #  Cloud Shell Editor is the ephemeral container that runs in your GCP project. You can access it via your Internet browser. You will also see a VSCode, as a convinient way to review configuration and modify stack and it\u0026rsquo;s components if needed. Basically you can use VSCode as your DevOps IDE.\nUse terminal for deployment: we use hub cli as a a single deployment tool that can switch between different deployment tools, native to each individual component and pass input and output parameters between them.\nEach Cloud Shell session will use a special Toolbox Image optimized to run as a Cloud Shell Editor container. It contains all necessary and pre-tested tools to execute Kubeflow deployment.\n Toolbox content can be found here: https://github.com/agilestacks/toolbox/tree/google/gcp-cloud-shell Pulled from GCR registry: gcr.io/superhub/cloud-shell  Primary tools used during the deployment:\n gcloud kubectl (with kustomze v4.0+) helm git  What happens during the deployment #  When you run a hub stack init Then hub will read a components described in hub.yaml file and download it. It will also prompt for GCP project. There are number of hub extensions (plugins) primarilly written in shell. You will find it in hub.yaml in the section called extensions\nWhen you run hub stack configure (or implicitly during hub stack deploy), then gcp extension will generate a new domain name and store it in the form of HUB_DOMAIN_NAME variable. Actually all variables has been pased to the hub.yaml via .env file. You don\u0026rsquo;t want to put your .env file in the git.\nThis domain name will be used by the hub as a natural ID of your deployment. If you have your own domain available as a DNS zone in GCP, then you can set it by running:\nhub stack configure --domain-name kubeflow.example.com TLS is a required by Kubeflow. It has been managed by cert-manager component. We use Lets Encrypt as the ACME provider.\n"},{"id":4,"href":"/technical-documentation/architecture/","title":"Architecture","section":"Technical Documentation","content":"Architecture #  Comming soon #  "},{"id":5,"href":"/technical-documentation/components/","title":"Components","section":"Technical Documentation","content":"Kubeflow Components #  Kubeflow features full ML lifecycle for model training, huperparameter tuning and model serving. Most noticable components of Kubeflow you can find below.\n   Component Description     Notebooks Jupyter notebooks web application   Pipelines Provides a Python DSL for DAG to run distributed training   Katib Hyperparameter tuning   TensorFlow Training Tensorflow training operator with TFJob custom resource   Multi Tenancy Provides multi-user isolation   Central Dashboard Common UI to access all Kubeflow applicaitons   ML Metadata Metadata store backed with relational database   Artifacts Store Object store (such as Minio) to store results of the trainings    The EPAM Kubeflow distribution also includes components to extend or improve ML lifecycle.\n   Component Description     Argo Workflows Pipeline workflow engine   Istio Ingress Gateway    Kubeflow Authn HTTP Filter HTTP filter for Istio (Envoy)   MinIO Object storage    "},{"id":6,"href":"/technical-documentation/components/argo-workflows/","title":"Argo Workflows","section":"Components","content":"Argo Workflows #  Overview of Argo Workflows #  Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD.\n Define workflows where each step in the workflow is a container. Model multi-step workflows as a sequence of tasks or capture the dependencies between tasks using a graph (DAG). Easily run compute intensive jobs for machine learning or data processing in a fraction of the time using Argo Workflows on Kubernetes. Run CI/CD pipelines natively on Kubernetes without configuring complex software development products.  Implementation Details #  The Argo Workflows Component has the following directory structure:\n./ ├── hub-component.yaml # configuration and parameters file of Hub component ├── values.yaml.template # template of Helm\u0026#39;s values.yaml file ├── post-deploy # script that is executed after deploy of the current component ├── post-undeploy # script that is executed after undeploy of the current component ├── pre-deploy # script that is executed before deploy of the current component ├── charts/ # directory for Helm Chart sources ├── resources/ # component custom resource descriptors └── backup # shell script that contains backup routines The component uses an offical Helm Chart to provision Argo Forkflows.\nTo make configuration more flexible, ingress object for Argo workflow dashboard and IAM is configured outside the Helm in post-deploy script.\nParameters #  The following component level parameters can be set in hub-component.yaml:\n   Name Description Default Value     component.argo.namespace Kubernetes namespace where Argo is provisioned kubeflow   component.argo.workflowNamespace Kubernetes namespace where workflow workloads are created kubeflow   component.argo.workflowRBAC Flag that enables k8s Role and RoleBinding creation in the workflowNamespace with required permissions to run workflow workloads true   component.argo.version Argo version v2.12.3   component.argo.containerRuntimeExecutor Container runtime, read more here k8sapi   component.argo.helm.chart Helm Chart version argo-workflows-0.9.4.tgz    Dependencies #  Argo Component does not depend on any other component in the stack\nReferences #   Argo Workflows Helm  "},{"id":7,"href":"/technical-documentation/components/authn/","title":"Authn","section":"Components","content":"Authn HTTP Filter #  Overview of the Authn HTTP Filter #  This is a HTTP filter for Istio (Envoy) that validates User session and redirects to dex for authentication (if invalid).\nImplementation Details #  The component has the following directory structure:\n./ ├── authn-envoy-filter.json.template # HTTP filter configuration ├── hub-component.yaml # Component definition ├── kustomization.yaml.template # Kustomize config └── oidc.yaml.template # Custom resource template for dex integration Parameters #  The following component level parameters has been defined hub-component.yaml:\n   Name Description Default Value     component.ingress.protocol HTTP or HTTPS schema https   component.dex.issuer OIDC auth URL (Dex) http://auth.${domain.name}   component.kubeflow.authn.oidcProvider Kubeflow OIDC auth URL https://kubeflow.${dns.domain}/login/oidc   component.kubeflow.authn.oidcSecret Hard to guess OIDC secret passphrase between Kubeflow and Dex (recommended: randomly generated string)    component.kubeflow.authn.sessionMaxAge Max age (in seconds) for user session 86400   component.istio.namespace Kubernetes namespace for Istio istio-system   component.istio.ingressGateway Name of Istio ingress gateway service     See Also #   Kubeflow Authn Design Article Istio documentation on Envoy Filters  "},{"id":8,"href":"/technical-documentation/components/central-dashboard/","title":"Central Dashboard","section":"Components","content":"Central Dashboard #  Overview of the Kubeflow Central Dashboard #  Central Dashboard is Kubeflow landing page. It provides following functionality:\n Web UI to access all Kubeflow components User registration flow  Implementation Details #  The component has the following directory structure:\n./ ├── deployment_patch.yaml # Kustomize patch that adds extra env vars for pod ├── hub-component.yaml # Parameters definitions ├── kustomization.yaml.template # Kustomize file for ths component ├── links-config.json # Configuration for splash screen ├── params.env.template # Configuration for environment variables of a central-dashboard pod ├── params.yaml # Config for Kustomize varibles ├── pre-deploy # Script to download tarball from kubeflow distribution website ├── pre-undeploy -\u0026gt; pre-deploy ├── clusterrole-binding.yaml.template # RBAC for cluster role bindings └── role-binding.yaml.template # RBAC for role bindings The component uses an offical Kubeflow distribution Kustomize scripts and applies patches and additional resources described in kustomize.yaml file.\nWhere pre-deploy script has been responsible for download tarball from Kubeflow official distribution website.\nThis component contains a special parameters to enable image pull from private docker registry\nBy default in the Kubeflow user id has been a valid email address. This is not the case for Intel, where user id is an IDSID parameter (from LDAP) which is not an email address. To allow this, we had to relax a user field validation in Add Contributor UI screen\nParameters #  The following component level parameters has been defined hub-component.yaml\n   Name Description Default Value     component.kubeflow.namespace Target Kubernetes namespace for this component kubeflow   component.kubeflow.dashboard.image Central dashboard docker image configuration gcr.io/kubeflow-images-public/centraldashboard   component.kubeflow.dashboard.imageTag Central dashboard docker image configuration vmaster-g8097cfeb   component.kubeflow.dashboard.contributorFormat REGEX to configure validation for profiles congtributor ^.+$   component.kubeflow.dashboard.contributorValidationMessage Custom error message for contributor validation ^.+$    See Also #   Central Dashboard official documentation Project source code on Github  "},{"id":9,"href":"/technical-documentation/components/dex/","title":"Dex","section":"Components","content":"DEX #  Overview of Argo Workflows #  Dex is an identity service that uses OpenID Connect to drive authentication for other apps.\nDex acts as a portal to other identity providers through “connectors.” This lets Dex defer authentication to LDAP servers, SAML providers, or established identity providers like GitHub, Google, and Active Directory. Clients write their authentication logic once to talk to Dex, then Dex handles the protocols for a given backend.\nImplementation Details #  The Argo Workflows Component has the following directory structure:\n./ ├── kubernetes # DEX related kubernetes resources and CRDs ├── hub-component.yaml # Component definition file ├── deploy.sh # Deploy script └── undeploy.sh # Undeloy script Parameters #  The following component level parameters can be set in hub-component.yaml:\n   Name Description Default Value     dns.domain Domain name of the kubeflow stack    component.ingress.protocol HTTP or HTTPS schema https   component.ingress.class Name of ingress class in kubernetes    component.ingress.ssoUrlPrefix Url prefix for protected applications dex   component.dex.name Name of Dex kubernetes resources dex   component.dex.namespace Name of kubernetes namespace where Dex will be deployed kube-system   component.dex.oidcIssuerFqdn Url of Dex issuer auth.${dns.domain}   component.dex.image Dex docker image dexidp/dex:v2.26.0   component.dex.passwordDb.email Static password DB user email    component.dex.passwordDb.password Static password DB user password    component.dex.ldap.host Host and optional port of the LDAP server in the form \u0026ldquo;host:port\u0026rdquo;    component.dex.ldap.dn The DN for an application service account    component.dex.ldap.usernamePrompt The attribute to display in the provided password prompt Username   component.dex.ldap.password The password for an application service account    component.dex.ldap.search.dn User search. BaseDN to start the search from    component.dex.ldap.search.usernameAttr Username attribute used for comparing user entries uid   component.dex.ldap.search.filter Optional filter to apply when searching the directory (objectClass=user)   component.dex.ldap.search.idAttr String representation of the user uid   component.dex.ldap.groupSearch.dn Group search. BaseDN to start the search from ${component.dex.ldap.search.dn}   component.dex.authproxy.image Docker image of ASI oauth2 proxy agilestacks/oauth2_proxy:v2.3   component.dex.authOperator.image Docker image of auth operator agilestacks/auth-operator:0.1.1   component.dex.authproxy.emailDomain Authenticate emails with the specified domain. Use * to authenticate any email '*'   component.dex.authproxy.cookieExpire Expire timeframe for cookie '12h0m0s'    References #   DEX ASI Auth operator ASI OAuth2 proxy  "},{"id":10,"href":"/technical-documentation/components/istio-ingress-gateway/","title":"Istio Ingress Gateway","section":"Components","content":"Istio Gateway #  Along with creating a service mesh, Istio allows you to manage gateways, which are Envoy proxies running at the edge of the mesh, providing fine-grained control over traffic entering and leaving the mesh.\nCurrent component will use a Helm chart to deploy an Istio Gateway\nParameters #     Name Description Default Value     hub.componentName Name of the istio ingress gateway    dns.domain Domain name of the kubeflow stack    component.istio.namespace Namespace of the istio istio-system   component.istio.version Version of Istio 1.5.9   component.istio.ingressGateway.replicas Amound of replicas 1   component.istio.ingressGateway.serviceType Kubernetes service type ClusterIP   component.istio.docker.repository Docker repository docker.io/istio   component.istio.chartDir Directory where to place helm chart .workdir   component.istio.chart Name of helm tarball file istio-${component.istio.version}.tgz   component.ingress.protocol HTTP or HTTPS schema http   component.ingress.class Name of ingress class in kubernetes    component.ingress.host Ingress host of kubernetes resource    component.ingress.maxUploadSize Ingress configuration 1024m   component.ingress.uploadTimeout Ingress configuration 1800   component.ingress.readTimeout Ingress configuration 1800    Output parameters #     Name Description     component.istio.ingressGateway Name of istio gateway   component.ingress.url URL of ingress   component.ingress.host Hostname of ingress    Implementation Specifics #  Helm chart downloaded from istio repo: https://storage.googleapis.com/istio-release/releases/$(ISTIO_VERSION)/charts/\nWe use a helm chart gateways, which is a dependency of the helm chart istio.\nValues files has been taken with with precedence to the later:\n Global values: Taken from istio/values.yaml only global; rest has been ignored Ingress gateway default values: Taken from istio/charts/gateways/values.yaml only istio-ingressgateway (renamed to hub.componentName); rest has been ignored Component values from: values.yaml.template  "},{"id":11,"href":"/technical-documentation/components/istio/","title":"Istio","section":"Components","content":"Istio #  Istio extends Kubernetes to establish a programmable, application-aware network using the powerful Envoy service proxy. Working with both Kubernetes and traditional workloads, Istio brings standard, universal traffic management, telemetry, and security to complex deployments.\nParameters #     Name Description Default Value     hub.componentName Name of the istio ingress gateway    dns.domain Domain name of the kubeflow stack    component.istio.namespace Kubernetes namespace where istio is deloyed istio-system   component.istio.chart.name Name of helm chart istio   component.istio.version Version of Istio 1.5.9    Output parameters #     Name Description     component.istio.namespace Name of istio gateeway    Implementation Specifics #  Helm chart downloaded from istio repo: https://storage.googleapis.com/istio-release/releases/$(ISTIO_VERSION)/charts/\n"},{"id":12,"href":"/technical-documentation/components/katib/","title":"Katib","section":"Components","content":"Katib #  Overview of the Katib #  Katib is a Kubernetes-native project for automated machine learning (AutoML). Katib supports hyperparameter tuning, early stopping and neural architecture search (NAS). Learn more about AutoML at fast.ai, Google Cloud, Microsoft Azure or Amazon SageMaker.\nImplementation Details #  The component has the following directory structure:\n./ ├── bin # Directory contains additional component hooks │ └── self-signed-ca.sh # Hook for generating self-signed certificates ├── crds # Directory contains custom resource definition files │ ├── experiment-crd.yaml # CRD of Experiment kind │ ├── suggestion-crd.yaml # CRD of Suggestion kind │ └── trial-crd.yaml # CRD of Trial kind ├── patches # Directory contains kustomize defined patches │ └── katib-controller-deployment.yaml # Kustomize patch ├── hub-component.yaml # Parameters definitions ├── kustomization.yaml.template # Kustomize file for ths component ├── post-undeploy # Script to delete certificates under directory .certs ├── pre-deploy # Script to download tarball from kubeflow distribution website and generate self-signed certs if no .certs directory ├── pre-undeploy # Script to download tarball from kubeflow distribution website and generate self-signed certs if no .certs directory └── secrets.env.template # Template with secrets defined as environment variables Parameters #  The following component level parameters has been defined hub-component.yaml\n   Name Description Default Value     dns.domain Domain name of the kubeflow stack    component.kubeflow.name Target Kubernetes resources name for this component    component.kubeflow.namespace Target Kubernetes namespace for this component    component.kubeflow.version Version of Kubeflow v1.2.0   component.kubeflow.tarball URL to kubeflow tarball archive https://github.com/kubeflow/manifests/archive/${component.kubeflow.version}.tar.gz   component.kubeflow.katib.controller.image Katib docker image docker.io/kubeflowkatib/katib-controller   component.kubeflow.katib.controller.imageTag Katib docker image tag v1beta1-a96ff59   component.kubeflow.katib.ui.image Katib UI docker image docker.io/kubeflowkatib/katib-ui   component.kubeflow.katib.ui.imageTag Katib UI docker image tag v1beta1-a96ff59   component.kubeflow.katib.dbManager.image Katib DB manager docker image docker.io/kubeflowkatib/katib-db-manager   component.kubeflow.katib.dbManager.imageTag Katib DB manager docker image tag v1beta1-a96ff59   component.mysql.host MySQL database host    component.mysql.port MySQL database port    component.mysql.user MySQL database user    component.mysql.password MySQL database password    component.mysql.database MySQL database database     See Also #   Katib official documentation Project source code on Github  "},{"id":13,"href":"/technical-documentation/components/notebooks/","title":"Notebooks","section":"Components","content":"Notebooks #  Overview of the Kubeflow Notebooks service #  A web application to allow user to create, update and delete Jupyter Notebooks inside their profile. This application can work as a standalone application, however by default it wll be opened from Kubeflow Central dashboard via iframe.\nRequirements #   Requires kustomize CLI. Doesn\u0026rsquo;t work with kubectl -k ...  Implementation Details #  This component will deploy two services of the Jupyter notebooks:\n jupyter-web-app - a web application notebook-controller - a BFF (backend-for-frontend) of this applicaiton.  Notebook creation form can be customized in jupyter-web-app/spawner_ui_config.yaml file.\n There was a special update to the notebook to allow user select GPUs from a dropdown.\n Once a notebook has been created. In the Kubernetes it has been represented as a Custom Resource (Notebook), so notebook controller also acts as an operator for this custom resource.\nThe component has the following directory structure:\n./ ├── crds │ └── notebook.yaml # CRD, contains previous verions, for smooth in-place upgrade ├── jupyter-web-app # Jupyter Web application (UI) │ ├── kustomization.yaml.template # Kustomize script for Jupyter web application │ ├── params.yaml # Application environment variables │ └── spawner_ui_config.yaml.template # Extracted notebook creation form config for easier customization ├── notebook-controller # Backend for UI and operator for Notebooks │ ├── kustomization.yaml.template # Kustomize script for Notebook │ └── params.env.template # Environment variables for backend ├── README.md ├── backup # Script for backups ├── deploy.sh # Customized deployment script to hook both kustomize files ├── hub-component.yaml # Hub manifest ├── post-deploy # Special addon to hook a optional restore from backup script └── undeploy.sh # Undeployment script for both kustomze applicaitons The component uses an offical Kubeflow distribution Kustomize scripts and applies patches and additional resources described in kustomize.yaml.template file for each service.\nWhere deploy script has been responsible for download tarball from Kubeflow official distribution website and deploy it.\nParameters #  The following component level parameters has been defined hub-component.yaml\n   Name Description Default Value     component.kubeflow.namespace Target Kubernetes namespace for this component kubeflow   component.kubeflow.dashboard.image Central dashboard docker image configuration gcr.io/kubeflow-images-public/centraldashboard   component.kubeflow.dashboard.imageTag Central dashboard docker image configuration vmaster-g8097cfeb   component.kubeflow.dashboard.contributorFormat REGEX to configure validation for profiles congtributor ^.+$   component.kubeflow.dashboard.contributorValidationMessage Custom error message for contributor validation ^.+$    See Also #   Central Dashboard on Kubeflow website  "},{"id":14,"href":"/technical-documentation/components/pipelines/","title":"Pipelines","section":"Components","content":"Pipelines #  Overview of the Kubeflow pipelines service #  Kubeflow is a machine learning (ML) toolkit that is dedicated to making deployments of ML workflows on Kubernetes simple, portable, and scalable.\nKubeflow pipelines are reusable end-to-end ML workflows built using the Kubeflow Pipelines SDK.\nThe Kubeflow pipelines service has the following goals:\n End to end orchestration: enabling and simplifying the orchestration of end to end machine learning pipelines Easy experimentation: making it easy for you to try numerous ideas and techniques, and manage your various trials/experiments. Easy re-use: enabling you to re-use components and pipelines to quickly cobble together end to end solutions, without having to re-build each time.  Enables data scientists to define data pipelines (DAG) using notebook and python. We support only Multi-User isolation.\nThis component uses argo as the driver for pipelines.\nDependency #  Depends on:\n argo: (Runtime for DAG). Argo dependency is transitive (via CRD) minio: (DAG artifacts) mysql-pipelines: persistence kubeflow-profiles: (backend for multi-user isolation) kubeflow-metadata: (model outputs)  Implementation Details #  ./ ├── crds # Pipeline CRD files ├── envs # Templates for kustomize parameters ├── patches # Templates to patch existing (original) kustomize resources ├── resources # Templates to replace existing (original) kustomize resources ├── hub-component.yaml # Component definition file ├── kustomization.yaml.template # Main kustomize template file ├── pre-deploy # Downloads tarball from kubeflow distribution webiste └── pre-undeploy -\u0026gt; pre-deploy This component uses Kustomize extension and follows common design guidelines for Kustomize components.\nParameters #     Name Description Default Value     component.kubeflow.manifests.version Version of kubeflow deployment manifests v1.2.0   component.kubeflow.pipeline.multiUser Set\u0026rsquo;s for multi user isolation true   component.mysql.host MySQL server host name    component.mysql.port MySQL server port (default to 3306) 3306   component.mysql.user MySQL server username (cannot be empty)    component.mysql.password MySQL server user password (cannot be empty)    component.mysql.database MySQL server database (cannot be empty)    component.bucket.endpoint Minio endpoint expected internal endpoint (cluster.local)    component.bucket.host Hostname part of of the endpoint    component.bucket.port Minio service port    component.bucket.region Minio region    component.bucket.accessKey Minio access key id    component.bucket.secretKey Minio secret access key     Changelog and TODOs #  Since Kubeflow v1.2 #   Introduced since kubeflow v1.2   NOTE: this component has been subject of heavy changes since Kubeflow v1.3 upgrade might not be backward compatible (on infrastructure as code level)\n See Also #   Kubeflow Pipelines overview  "},{"id":15,"href":"/technical-documentation/components/seldon-core/","title":"Seldon Core","section":"Components","content":"Seldon Core #  Overview of the Seldon Core #  Implementation Details #  The component has the following directory structure:\n./ ├── charts # Directory contains helm charts archives ├── deploy.sh # Script to backup mysql data ├── hub-component.yaml # Parameters definitions ├── istio-gateway.yaml.template # Parameters definitions ├── undeploy.sh # Post deploy script to restore data from backup file if provided └── values.yaml.template # Base helm values template Parameters #  The following component level parameters has been defined hub-component.yaml\n   Name Description Default Value     dns.domain Domain name of the kubeflow stack    component.seldon.namespace  seldon-system   component.seldon.version  1.5.0   executor.resources.cpuLimit  500m   executor.resources.cpuRequest  500m   executor.resources.memoryLimit  512Mi   executor.resources.memoryRequest  512Mi   manager.cpuLimit  500m   manager.cpuRequest  100m   manager.memoryLimit  300Mi   manager.memoryRequest  200Mi   storageInitializer.cpuLimit  \u0026quot;1\u0026quot;   storageInitializer.cpuRequest  100m   storageInitializer.memoryLimit  1Gi   storageInitializer.memoryRequest  100Mi   engine.resources.cpuLimit  500m   engine.resources.cpuRequest  500m   engine.resources.memoryLimit  512Mi   engine.resources.memoryRequest  512Mi   component.seldon.docker.registry  docker.io   component.seldon.docker.operator.repository  seldonio/seldon-core-operator   component.seldon.docker.executor.repository  seldonio/seldon-core-executor   component.seldon.docker.engine.repository  seldonio/engine   component.seldon.docker.mlflow.image  seldonio/mlflowserver   component.seldon.docker.mlflow.tag  \u0026quot;${component.seldon.version}\u0026quot;   component.seldon.docker.sklearn.image  seldonio/sklearnserver   component.seldon.docker.sklearn.tag  \u0026quot;${component.seldon.version}\u0026quot;   component.seldon.docker.kfserving.image  seldonio/mlserver   component.seldon.docker.kfserving.tag  0.1.1   component.seldon.docker.tfServingProxy.image  seldonio/tfserving-proxy   component.seldon.docker.tfServingProxy.tag  \u0026quot;${component.seldon.version}\u0026quot;   component.seldon.docker.tfServing.image  tensorflow/serving   component.seldon.docker.tfServing.tag  2.1.0   component.seldon.docker.xgboost.image  seldonio/xgboostserver   component.seldon.docker.xgboost.tag  \u0026quot;${component.seldon.version}\u0026quot;   component.seldon.docker.alibiExplainer.image  seldonio/alibiexplainer   component.seldon.docker.alibiExplainer.tag  \u0026quot;${component.seldon.version}\u0026quot;   component.istio.namespace  istio-system   component.istio.ingressGateway  istio-ingressgateway   component.ingress.host      Outputs #     Name Description     component.seldon.istioGateway Default istio gateway for seldon deployments    See Also #  "},{"id":16,"href":"/technical-documentation/requirements/gke/access-rights/","title":"Access Rights","section":"GKE","content":"Access rights #  Comming soon #  "},{"id":17,"href":"/technical-documentation/requirements/gke/addons/","title":"Addons","section":"GKE","content":"GKE Addons (WIP) #  In order to deploy EPAM Kubeflow distribution to GKE next addons should be enabled:\n NetworkPolicy NodeLocalDNS ConfigConnector  "},{"id":18,"href":"/technical-documentation/requirements/gke/sufficient-capacity/","title":"Sufficient Capacity","section":"GKE","content":"Sufficient capacity #  Comming soon #  "},{"id":19,"href":"/user-guides/how-can-i-run-pipeline-from-my-machine/","title":"How Can I Run Pipeline From My Machine","section":"User-guides","content":"How can I run pipeline from my machine #  Comming soon #  "},{"id":20,"href":"/features/auto-ml/","title":"AutoML","section":"Features","content":"AutoML (Katib) #  Katib is a Kubernetes-native project for automated machine learning (AutoML). Katib supports hyperparameter tuning, early stopping and neural architecture search (NAS). Learn more about AutoML at fast.ai, Google Cloud, Microsoft Azure or Amazon SageMaker.\nComming soon #  "},{"id":21,"href":"/getting-started/faq/","title":"FAQ","section":"Getting Started","content":"FAQ #  Comming soon #  "},{"id":22,"href":"/getting-started/deploy-kubeflow-on-cloud/deploy-on-gke/","title":"Kubeflow on GKE","section":"Deploy Kubeflow on Cloud","content":"Kubeflow on GKE #  Here we explain how to deploy a Kubeflow into your Google Cloud Platform environment\nDeployment Prerequisites #   You should be signed in and have an active project in GCP: https://console.cloud.google.com You should have already deployed a GKE cluster.   Don\u0026rsquo;t have a cluster? Not a problem. We have a GKE stack for you, follow this link\n Deployment #  Deployment via Cloud Shell #  The easiest way how to get started is via Cloud Shell Edititor. Choose from the list of available Kubeflow stacks\n   Kubeflow Stack Description Link     Kubeflow cluster v1.2 This is our latest Kubeflow stack available     Start a new cloud shell session #  What will happen when you click to the button:\n You will start a new Cloud Shell editor session (best works with the Chrome Browser) Cloud shell will use a Shellbox image with all tools needed to deploy a kubeflow. No additional configuration required, See shellbox on gcr Cloud shell will clone a git repo with the stack files: See github repo: kubeflow-stacks  Init stack #  First you need initialize the sandbox configuration. To do this please run the initialization command:\nhub stack init The command will:\n Download the components described in hub.yaml Configure this Cloud Shell session for your GCP. It will ask other GCP essentials. It will take Region and Location (zone) from GCP metadat aserver. You can change settings by modifying .env file  Configure and Deploy a stack #  Once you are done with the configuration, use the following command to deploy the sandbox:\nhub stack deploy Before the first deployment you will be prompted for few questions. Your settings will be captured in .env file.\n Name of GKE cluster in your region. You will see a list of already deployed GKE clusters in your region. For different region, please adjust variables in .env file and thern run hub stack deploy command again Name of the storage class (more info here) GKE provides several storage classes you will see them Other questions\u0026hellip;  Once all configuration settings completed it will start the deployment automatically\nReview configuration parameters before deployment #  Wait, not so fast! I want to review coniguration settings (and possibly adjust) before the deployment.\nIn this case, run the following command\nhub stack configure This command will only do a configuration step and save results in .env file. So you can review and adjust. Once you are happy then run\nhub stack deploy Clean Up #  To remove your Kubeflow deployment, please run the following command\nhub stack undeploy "},{"id":23,"href":"/technical-documentation/components/minio/","title":"MinIO","section":"Components","content":"MinIO #  Overview of the MinIO #  MinIO is a Kubernetes-native object store designed to provide high performance with an S3-compatible API.\nImplementation Details #  The component has the following directory structure:\n./ ├── hub-component.yaml # Component definition ├── values-ingress.yaml.gotemplate # Helm chart ingress value template └── values.yaml.template # Helm chart value template Parameters #  The following component level parameters has been defined hub-component.yaml:\n   Name Description Default Value     component.ingress.protocol HTTP or HTTPS schema http   component.ingress.host Ingress host ${hub.componentName}.${dns.domain}   component.ingress.class Name of ingress class in kubernetes    component.bucket.region Storage bucket region us-east-1   component.bucket.name Storage bucket name default   component.storage-class.name Name of kubernetes storage class default   component.minio.accessKey MinIO access key    component.minio.secretKey MinIO secret key    component.minio.namespace Name of kubernetes namesapce where to deploy MinIO minio   component.minio.replicas Amount of MinIO replicas 4   component.minio.volumeType Persistence volume type gp2   component.minio.existingClaim A manually managed Persistent Volume and Claim    component.minio.storageSize Storage size 20Gi   component.minio.requests.memory Resource memory request 4Gi   component.minio.mode MinIO mode, i.e. standalone or distributed or gateway distributed   component.minio.requests.ram  1Gi   helm.repo Helm chart repository URL https://charts.min.io/   helm.chart Helm chart name minio   helm.version Helm chart version 4.0.2   helm.baseValuesFile Helm base values file values.yaml   helm.logLevel Helm log level info   docker.image MinIO docker image quay.io/minio/minio   docker.tag MinIO docker image tag RELEASE.2022-06-02T02-11-04Z   docker.mcImage MinIO client docker image quay.io/minio/mc   docker.mcTag MinIO client docker image tag    docker.jqImage JQ docker image bskim45/helm-kubectl-jq   docker.jqTag JQ docker image tag 3.1.0    See Also #   MinIO MinIO documentation for Hybrid Cloud  "},{"id":24,"href":"/technical-documentation/components/ml-metadata/","title":"ML Metadata","section":"Components","content":"ML Metadata #  Overview of the ML Metadata #  Kubeflow Pipelines backend stores runtime information of a pipeline run in Metadata store. Runtime information includes the status of a task, availability of artifacts, custom properties associated with Execution or Artifact, etc. Learn more at ML Metadata Get Started.\nImplementation Details #  The component has the following directory structure:\n./ ├── db-params.env.template # Database connection parameters template ├── db-secrets.env.template # Database connection secrets template ├── hub-component.yaml # Parameters definitions ├── kustomization.yaml.template # Kustomize file for ths component ├── pre-deploy # Script to download tarball from kubeflow distribution website and generate self-signed certs if no .certs directory └── pre-undeploy # Script to download tarball from kubeflow distribution website and generate self-signed certs if no .certs directory Parameters #  The following component level parameters has been defined hub-component.yaml\n   Name Description Default Value     dns.domain Domain name of the kubeflow stack    component.kubeflow.name Target Kubernetes resources name for this component    component.kubeflow.namespace Target Kubernetes namespace for this component    component.kubeflow.version Version of Kubeflow v1.2.0   component.kubeflow.tarball URL to kubeflow tarball archive https://github.com/kubeflow/manifests/archive/${component.kubeflow.version}.tar.gz   component.mysql.host MySQL database host    component.mysql.port MySQL database port 3306   component.mysql.user MySQL database user root   component.mysql.password MySQL database password    component.mysql.database MySQL database database metadb   component.mysql.emptyPassword Flag indicate that password is empty #{size(component.mysql.password) == 0}    See also #   ML Metadata Source code of ML Metadata  "},{"id":25,"href":"/getting-started/examples/mnist-example/","title":"MNIST example","section":"Examples","content":"MNIST example #  Comming soon #  "},{"id":26,"href":"/technical-documentation/components/mysql/","title":"MySQL","section":"Components","content":"MySQL #  Overview of the MySQL #  MySQL is a fast, reliable, scalable, and easy to use open source relational database system. Designed to handle mission-critical, heavy-load production applications.\nImplementation Details #  The component has the following directory structure:\n./ ├── charts # Directory contains helm charts archives ├── backup # Script to backup mysql data ├── hub-component.yaml # Parameters definitions ├── post-deploy # Post deploy script to restore data from backup file if provided ├── values-auth.yaml.template # MySQL auth helm values template └── values.yaml.template # Base helm values template Parameters #  The following component level parameters has been defined hub-component.yaml\n   Name Description Default Value     dns.domain Domain name of the kubeflow stack    component.storage-class.name     component.mysql.namespace  mysql   component.mysql.port  3306   component.mysql.user MySQL database user    component.mysql.password MySQL database password    component.mysql.database MySQL database name    component.mysql.rootPassword MySQL database root password    component.mysql.volumeSize Storage size 8Gi   component.mysql.docker.registry Docker image registry docker.io   component.mysql.docker.repository Docker image repository bitnami/mysql   component.mysql.docker.tag Docker image tag 8.0.22-debian-10-r23   component.mysql.helm.chart File name of helm chart mysql-8.0.0.tgz   component.mysql.helm.valuesFile Name of helm values file values.yaml   hub.backup.file Hub CLI backup file name     Outputs #     Name Description     component.mysql.host MySQL database host   component.mysql.port MySQL database port   component.mysql.database MySQL database database   component.mysql.user MySQL database user   component.mysql.password MySQL database password   component.mysql.rootPassword MySQL database root password    See Also #   MySQL  "},{"id":27,"href":"/technical-documentation/components/tensorflow-training/","title":"TensorFlow Training","section":"Components","content":"TensorFlow Training #  Overview of the TensorFlow Training #  TFJob is a Kubernetes custom resource to run TensorFlow training jobs on Kubernetes. The Kubeflow implementation of TFJob is in training-operator.\nImplementation Details #  The component has the following directory structure:\n./ ├── crds # Directory contains custom resource definition files │ └── tfjobs.yaml # CRD of TFJob kind ├── hub-component.yaml # Parameters definitions ├── kustomization.yaml.template # Kustomize file for ths component ├── pre-deploy # Script to download tarball from kubeflow distribution website └── pre-undeploy # Script to download tarball from kubeflow distribution website Parameters #  The following component level parameters has been defined hub-component.yaml\n   Name Description Default Value     dns.domain Domain name of the kubeflow stack    component.kubeflow.name Target Kubernetes resources name for this component    component.kubeflow.namespace Target Kubernetes namespace for this component    component.kubeflow.version Version of Kubeflow v1.2.0   component.kubeflow.tarball URL to kubeflow tarball archive https://github.com/kubeflow/manifests/archive/${component.kubeflow.version}.tar.gz    See also #   TensorFlow training overview TesorFlow  "}]